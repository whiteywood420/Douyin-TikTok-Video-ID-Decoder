#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ–éŸ³ / TikTok è§†é¢‘ ID è§£å¯†ç®—æ³• (Snowflake é£æ ¼)
Douyin / TikTok Video ID Decoder (Snowflake Style)

ä½œè€… / Author: Evil0ctal

è¯´æ˜ / Description:
  - æŠ–éŸ³ä¸ TikTok çš„è§†é¢‘ IDï¼ˆaweme_idï¼‰æ˜¯ 64 ä½æ— ç¬¦å·æ•´æ•°ã€‚
    Douyin and TikTok video IDs (aweme_id) are 64-bit unsigned integers.
  - é«˜ 32 ä½ = Unix æ—¶é—´æˆ³ï¼ˆç§’çº§ï¼Œå•ä½ï¼šsï¼‰
    High 32 bits = Unix timestamp (second-level, unit: s)
  - ä½ 32 ä½ = å”¯ä¸€æ€§ä½ï¼ˆåˆ†ç‰‡ID + åºåˆ—å·ï¼‰
    Low 32 bits = Uniqueness bits (shard ID + sequence number)
  - ç»“æ„ä¸ Twitter çš„ Snowflake ç±»ä¼¼ï¼Œåªæ˜¯æ—¶é—´ç²¾åº¦ä¸åŒï¼ˆSnowflake æ˜¯æ¯«ç§’çº§ï¼‰
    Similar to Twitter's Snowflake structure, but with different time precision (Snowflake uses milliseconds)
"""

import json
import secrets
import zoneinfo
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any

# è®¾ç½®æ—¶åŒºï¼ˆå¯è‡ªè¡Œæ›´æ”¹ï¼‰
# Set timezone (can be customized)
LA_TZ = zoneinfo.ZoneInfo("America/Los_Angeles")


def decode_aweme_id(id_str: str, analyze_low32: bool = False) -> Dict[str, Any]:
    """
    è§£ç æŠ–éŸ³ / TikTok è§†é¢‘ IDï¼ˆåè¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
    Decode Douyin / TikTok video ID (decimal string)

    :param id_str: è§†é¢‘ IDï¼ˆåè¿›åˆ¶å­—ç¬¦ä¸²ï¼‰/ Video ID (decimal string)
    :param analyze_low32: æ˜¯å¦æ·±åº¦åˆ†æä½32ä½ç»“æ„ / Whether to analyze low 32-bit structure in depth
    :return: åŒ…å«æ—¶é—´æˆ³ã€æ—¥æœŸã€ä½ä½å­—æ®µçš„è¯¦ç»†å­—å…¸ / Detailed dictionary containing timestamp, date, and low-bit fields
    """
    n = int(id_str)
    ts_sec = n >> 32  # é«˜ 32 ä½ï¼šç§’çº§æ—¶é—´æˆ³ / High 32 bits: second-level timestamp
    low32 = n & 0xFFFFFFFF  # ä½ 32 ä½ï¼šå”¯ä¸€æ€§ä½ï¼ˆæœºå™¨ + åºåˆ—å·ï¼‰/ Low 32 bits: uniqueness bits (machine + sequence)

    dt_utc = datetime.fromtimestamp(ts_sec, tz=timezone.utc)
    dt_la = dt_utc.astimezone(LA_TZ)

    result = {
        "id": id_str,
        "timestamp_sec": str(ts_sec),
        "datetime_utc": dt_utc.isoformat(),
        "datetime_America/Los_Angeles": dt_la.isoformat(),
        "low32_dec": str(low32),
        "low32_hex": hex(low32),
        "low32_bin": bin(low32),
    }

    # æ·±åº¦åˆ†æä½32ä½çš„å¯èƒ½ç»“æ„
    # Deep analysis of possible low 32-bit structures
    if analyze_low32:
        # å°è¯•ä¸åŒçš„ä½åˆ’åˆ†æ–¹æ¡ˆ
        # Try different bit division schemes

        # æ–¹æ¡ˆ1: æ ‡å‡† Snowflake (10+10+12)
        # Scheme 1: Standard Snowflake (10+10+12)
        scheme1 = {
            "datacenter_id": (low32 >> 22) & 0x3FF,  # é«˜10ä½ / High 10 bits
            "worker_id": (low32 >> 12) & 0x3FF,      # ä¸­10ä½ / Middle 10 bits
            "sequence": low32 & 0xFFF,                # ä½12ä½ / Low 12 bits
        }

        # æ–¹æ¡ˆ2: ä¿®æ”¹ç‰ˆ (8+8+16)
        # Scheme 2: Modified version (8+8+16)
        scheme2 = {
            "datacenter_id": (low32 >> 24) & 0xFF,   # é«˜8ä½ / High 8 bits
            "worker_id": (low32 >> 16) & 0xFF,       # ä¸­8ä½ / Middle 8 bits
            "sequence": low32 & 0xFFFF,               # ä½16ä½ / Low 16 bits
        }

        # æ–¹æ¡ˆ3: ç®€åŒ–ç‰ˆ (16+16)
        # Scheme 3: Simplified version (16+16)
        scheme3 = {
            "shard_id": (low32 >> 16) & 0xFFFF,      # é«˜16ä½ / High 16 bits
            "sequence": low32 & 0xFFFF,               # ä½16ä½ / Low 16 bits
        }

        # æ–¹æ¡ˆ4: å­—èŠ‚åˆ’åˆ† (8+8+8+8)
        # Scheme 4: Byte division (8+8+8+8)
        scheme4 = {
            "byte3": (low32 >> 24) & 0xFF,
            "byte2": (low32 >> 16) & 0xFF,
            "byte1": (low32 >> 8) & 0xFF,
            "byte0": low32 & 0xFF,
        }

        result["low32_analysis"] = {
            "scheme1_10_10_12": scheme1,
            "scheme2_8_8_16": scheme2,
            "scheme3_16_16": scheme3,
            "scheme4_bytes": scheme4,
        }

    return result


def forge_aweme_like_id(ts_sec: int, low32: int | None = None) -> int:
    """
    æ„é€ ä¸€ä¸ª"ç±»æŠ–éŸ³é£æ ¼"çš„è§†é¢‘ IDï¼ˆç”¨äºæµ‹è¯•éªŒè¯ï¼‰
    Forge a "Douyin-style" video ID (for testing and verification)

    :param ts_sec: ç§’çº§æ—¶é—´æˆ³ï¼ˆintï¼‰/ Second-level timestamp (int)
    :param low32: è‡ªå®šä¹‰ä½ 32 ä½æ•´æ•°ï¼ˆå¯é€‰ï¼‰/ Custom low 32-bit integer (optional)
    :return: æ„é€ å‡ºçš„ 64 ä½æ•´æ•° / Constructed 64-bit integer
    """
    if low32 is None:
        low32 = secrets.randbits(32)
    return (ts_sec << 32) | (low32 & 0xFFFFFFFF)


def batch_decode(ids: List[str]) -> None:
    """
    æ‰¹é‡è§£ç å¤šä¸ªè§†é¢‘ ID
    Batch decode multiple video IDs
    """
    for i, aweme_id in enumerate(ids, start=1):
        result = decode_aweme_id(aweme_id)
        print(f"\n[{i}] ID: {aweme_id}")
        print(f"  å‘å¸ƒæ—¶é—´(UTC): {result['datetime_utc']}")
        print(f"  å‘å¸ƒæ—¶é—´(LA):  {result['datetime_America/Los_Angeles']}")
        print(f"  æ—¶é—´æˆ³ç§’æ•°:   {result['timestamp_sec']}")
        print(f"  ä½32ä½(åè¿›åˆ¶): {result['low32_dec']}")
        print(f"  ä½32ä½(åå…­è¿›åˆ¶): {result['low32_hex']}")
        print(f"  ä½32ä½(äºŒè¿›åˆ¶): {result['low32_bin']}")


def deep_analyze_low32(ids: List[str]) -> None:
    """
    æ·±åº¦åˆ†æä½32ä½çš„ç»“æ„ï¼Œå¯»æ‰¾è§„å¾‹
    Deep analysis of low 32-bit structure to find patterns
    """
    print("\n" + "=" * 80)
    print("=== ä½32ä½æ·±åº¦åˆ†æ / Low 32-bit Deep Analysis ===")
    print("=" * 80)

    for i, aweme_id in enumerate(ids, start=1):
        result = decode_aweme_id(aweme_id, analyze_low32=True)
        analysis = result["low32_analysis"]

        print(f"\n[{i}] ID: {aweme_id}")
        print(f"  ä½32ä½: {result['low32_bin']} ({result['low32_hex']})")
        print(f"\n  æ–¹æ¡ˆ1 (æ ‡å‡† Snowflake: 10+10+12 ä½) / Scheme 1 (Standard Snowflake: 10+10+12 bits)")
        print(f"    æ•°æ®ä¸­å¿ƒID / Datacenter ID: {analysis['scheme1_10_10_12']['datacenter_id']:4d} (0x{analysis['scheme1_10_10_12']['datacenter_id']:03x})")
        print(f"    æœºå™¨ID / Worker ID:        {analysis['scheme1_10_10_12']['worker_id']:4d} (0x{analysis['scheme1_10_10_12']['worker_id']:03x})")
        print(f"    åºåˆ—å· / Sequence:          {analysis['scheme1_10_10_12']['sequence']:4d} (0x{analysis['scheme1_10_10_12']['sequence']:03x})")

        print(f"\n  æ–¹æ¡ˆ2 (ä¿®æ”¹ç‰ˆ: 8+8+16 ä½) / Scheme 2 (Modified: 8+8+16 bits)")
        print(f"    æ•°æ®ä¸­å¿ƒID / Datacenter ID: {analysis['scheme2_8_8_16']['datacenter_id']:4d} (0x{analysis['scheme2_8_8_16']['datacenter_id']:02x})")
        print(f"    æœºå™¨ID / Worker ID:        {analysis['scheme2_8_8_16']['worker_id']:4d} (0x{analysis['scheme2_8_8_16']['worker_id']:02x})")
        print(f"    åºåˆ—å· / Sequence:          {analysis['scheme2_8_8_16']['sequence']:6d} (0x{analysis['scheme2_8_8_16']['sequence']:04x})")

        print(f"\n  æ–¹æ¡ˆ3 (ç®€åŒ–ç‰ˆ: 16+16 ä½) / Scheme 3 (Simplified: 16+16 bits)")
        print(f"    åˆ†ç‰‡ID / Shard ID:          {analysis['scheme3_16_16']['shard_id']:6d} (0x{analysis['scheme3_16_16']['shard_id']:04x})")
        print(f"    åºåˆ—å· / Sequence:          {analysis['scheme3_16_16']['sequence']:6d} (0x{analysis['scheme3_16_16']['sequence']:04x})")

        print(f"\n  æ–¹æ¡ˆ4 (å­—èŠ‚åˆ’åˆ†: 8+8+8+8 ä½) / Scheme 4 (Byte division: 8+8+8+8 bits)")
        print(f"    å­—èŠ‚3 (é«˜) / Byte3 (High):  {analysis['scheme4_bytes']['byte3']:3d} (0x{analysis['scheme4_bytes']['byte3']:02x})")
        print(f"    å­—èŠ‚2 / Byte2:              {analysis['scheme4_bytes']['byte2']:3d} (0x{analysis['scheme4_bytes']['byte2']:02x})")
        print(f"    å­—èŠ‚1 / Byte1:              {analysis['scheme4_bytes']['byte1']:3d} (0x{analysis['scheme4_bytes']['byte1']:02x})")
        print(f"    å­—èŠ‚0 (ä½) / Byte0 (Low):   {analysis['scheme4_bytes']['byte0']:3d} (0x{analysis['scheme4_bytes']['byte0']:02x})")


def statistical_analysis(ids: List[str], platform_labels: List[str] = None) -> None:
    """
    ç»Ÿè®¡åˆ†æï¼šæ‰¾å‡ºIDä¹‹é—´çš„ç›¸å…³æ€§å’Œæ¨¡å¼
    Statistical analysis: Find correlations and patterns between IDs

    :param ids: è§†é¢‘IDåˆ—è¡¨ / List of video IDs
    :param platform_labels: å¹³å°æ ‡ç­¾åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚ ['Douyin', 'TikTok']ï¼‰/ Platform label list (optional, e.g., ['Douyin', 'TikTok'])
    """
    print("\n" + "=" * 80)
    print("=== ç»Ÿè®¡åˆ†æä¸æ¨¡å¼è¯†åˆ« / Statistical Analysis and Pattern Recognition ===")
    print("=" * 80)

    # æ”¶é›†æ•°æ® / Collect data
    data = []
    for aweme_id in ids:
        result = decode_aweme_id(aweme_id, analyze_low32=True)
        data.append(result)

    # åˆ†ææ–¹æ¡ˆ3 (16+16ä½) - æœ€å¯èƒ½çš„æ–¹æ¡ˆ
    # Analyze Scheme 3 (16+16 bits) - Most likely scheme
    print("\nã€æ¨èæ–¹æ¡ˆ / Recommended Schemeã€‘16+16ä½åˆ’åˆ† (åˆ†ç‰‡ID + åºåˆ—å·) / 16+16 bit division (Shard ID + Sequence)")
    print("-" * 80)

    # ç»Ÿè®¡åºåˆ—å·åˆ†å¸ƒ / Count sequence distribution
    sequence_counts = defaultdict(list)
    shard_counts = defaultdict(list)

    for i, d in enumerate(data):
        seq = d["low32_analysis"]["scheme3_16_16"]["sequence"]
        shard = d["low32_analysis"]["scheme3_16_16"]["shard_id"]
        label = platform_labels[i] if platform_labels else f"ID{i+1}"

        sequence_counts[seq].append((label, d["id"]))
        shard_counts[shard].append((label, d["id"]))

    # æ‰¾å‡ºé‡å¤çš„åºåˆ—å· / Find duplicate sequences
    print("\n1ï¸âƒ£  åºåˆ—å·é‡å¤æ£€æµ‹ (å¯èƒ½æ¥è‡ªåŒä¸€æ‰¹æ¬¡/æœåŠ¡å™¨) / Sequence Duplication Detection (may be from same batch/server):")
    duplicates = {k: v for k, v in sequence_counts.items() if len(v) > 1}
    if duplicates:
        for seq, ids_list in duplicates.items():
            print(f"\n   åºåˆ—å· / Sequence {seq} (0x{seq:04x}) å‡ºç° / Appears {len(ids_list)} æ¬¡ / times:")
            for label, vid in ids_list:
                print(f"     - [{label}] {vid}")
    else:
        print("   âœ“ æ‰€æœ‰åºåˆ—å·å”¯ä¸€ / All sequences are unique")

    # åˆ†ç‰‡IDç»Ÿè®¡ / Shard ID statistics
    print("\n2ï¸âƒ£  åˆ†ç‰‡IDåˆ†å¸ƒ / Shard ID Distribution:")
    for shard, ids_list in sorted(shard_counts.items()):
        if len(ids_list) > 1:
            print(f"\n   åˆ†ç‰‡ / Shard {shard:5d} (0x{shard:04x}): {len(ids_list)} ä¸ªè§†é¢‘ / videos")
            for label, vid in ids_list:
                print(f"     - [{label}] {vid}")
        else:
            label, vid = ids_list[0]
            print(f"   åˆ†ç‰‡ / Shard {shard:5d} (0x{shard:04x}): [{label}] {vid}")

    # æ—¶é—´åˆ†å¸ƒ / Time distribution
    print("\n3ï¸âƒ£  æ—¶é—´åˆ†å¸ƒ / Time Distribution:")
    timestamps = [(d["timestamp_sec"], d["id"]) for d in data]
    timestamps.sort()

    print(f"   æœ€æ—© / Earliest: {timestamps[0][0]} â†’ {data[0]['datetime_utc']}")
    print(f"   æœ€æ™š / Latest: {timestamps[-1][0]} â†’ {data[-1]['datetime_utc']}")

    time_diff = int(timestamps[-1][0]) - int(timestamps[0][0])
    print(f"   æ—¶é—´è·¨åº¦ / Time span: {time_diff} ç§’ / seconds ({time_diff/3600:.2f} å°æ—¶ / hours / {time_diff/86400:.2f} å¤© / days)")

    # ä½å­—èŠ‚åˆ†æ / Low byte analysis
    print("\n4ï¸âƒ£  ä½å­—èŠ‚ (Byte0) æ¨¡å¼åˆ†æ / Low Byte (Byte0) Pattern Analysis:")
    byte0_counts = defaultdict(list)
    for i, d in enumerate(data):
        byte0 = d["low32_analysis"]["scheme4_bytes"]["byte0"]
        label = platform_labels[i] if platform_labels else f"ID{i+1}"
        byte0_counts[byte0].append(label)

    for byte0, labels in sorted(byte0_counts.items()):
        if len(labels) > 1:
            print(f"   0x{byte0:02x} ({byte0:3d}): {', '.join(labels)} â­ é‡å¤ / Duplicate!")
        else:
            print(f"   0x{byte0:02x} ({byte0:3d}): {labels[0]}")

    # å¹³å°å¯¹æ¯”ï¼ˆå¦‚æœæä¾›äº†æ ‡ç­¾ï¼‰/ Platform comparison (if labels provided)
    if platform_labels:
        print("\n5ï¸âƒ£  å¹³å°å¯¹æ¯” / Platform Comparison:")
        platform_data = defaultdict(list)
        for i, d in enumerate(data):
            platform = platform_labels[i]
            platform_data[platform].append(d)

        for platform, pdata in platform_data.items():
            print(f"\n   ã€{platform}ã€‘ ({len(pdata)} ä¸ªè§†é¢‘ / videos)")
            seqs = [d["low32_analysis"]["scheme3_16_16"]["sequence"] for d in pdata]
            shards = [d["low32_analysis"]["scheme3_16_16"]["shard_id"] for d in pdata]

            print(f"     åºåˆ—å·èŒƒå›´ / Sequence range: {min(seqs)} - {max(seqs)}")
            print(f"     åˆ†ç‰‡IDèŒƒå›´ / Shard ID range:  {min(shards)} - {max(shards)}")
            print(f"     å”¯ä¸€åºåˆ—å· / Unique sequences:  {len(set(seqs))} / {len(seqs)}")
            print(f"     å”¯ä¸€åˆ†ç‰‡ID / Unique shards:  {len(set(shards))} / {len(shards)}")


def load_test_data(json_path: str = "aweme_ids_output.json") -> Dict[str, Any]:
    """
    åŠ è½½æµ‹è¯•æ•°æ®JSONæ–‡ä»¶
    Load test data JSON file

    :param json_path: JSONæ–‡ä»¶è·¯å¾„ / JSON file path
    :return: è§£æåçš„JSONæ•°æ® / Parsed JSON data
    """
    file_path = Path(__file__).parent / json_path
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def validate_decode_algorithm(json_path: str = "aweme_ids_output.json") -> None:
    """
    ä½¿ç”¨çœŸå®æ•°æ®éªŒè¯è§£ç ç®—æ³•çš„å‡†ç¡®æ€§
    Validate decode algorithm accuracy using real data

    :param json_path: æµ‹è¯•æ•°æ®JSONæ–‡ä»¶è·¯å¾„ / Test data JSON file path
    """
    print("\n" + "=" * 80)
    print("=== è§£ç ç®—æ³•å‡†ç¡®æ€§éªŒè¯ / Decode Algorithm Accuracy Validation ===")
    print("=" * 80)

    # åŠ è½½æµ‹è¯•æ•°æ® / Load test data
    test_data = load_test_data(json_path)
    videos = test_data["videos"]

    print(f"\nåŠ è½½æµ‹è¯•æ•°æ® / Loading test data:")
    print(f"  æ€»æ•°é‡ / Total count: {test_data['total_count']}")
    print(f"  æŠ–éŸ³ / Douyin:   {test_data['douyin_count']}")
    print(f"  TikTok: {test_data['tiktok_count']}")

    # ç»Ÿè®¡æ•°æ® / Statistical data
    results = {
        "total": 0,
        "exact_match": 0,
        "close_match": 0,  # è¯¯å·®åœ¨5ç§’å†… / Error within 5 seconds
        "time_diffs": [],
        "douyin_diffs": [],
        "tiktok_diffs": [],
    }

    print("\n" + "-" * 80)
    print("å¼€å§‹éªŒè¯ / Starting validation...")
    print("-" * 80)

    for video in videos:
        aweme_id = video["aweme_id"]
        actual_timestamp = video["create_time"]
        source = video["source"]
        create_datetime = video["create_datetime"]

        # è§£ç ID / Decode ID
        decoded = decode_aweme_id(aweme_id)
        decoded_timestamp = int(decoded["timestamp_sec"])

        # è®¡ç®—æ—¶é—´å·® / Calculate time difference
        time_diff = decoded_timestamp - actual_timestamp

        # æ”¶é›†ç»Ÿè®¡ / Collect statistics
        results["total"] += 1
        results["time_diffs"].append(abs(time_diff))

        if source == "Douyin":
            results["douyin_diffs"].append(time_diff)
        else:
            results["tiktok_diffs"].append(time_diff)

        # åˆ†ç±» / Classify
        if time_diff == 0:
            results["exact_match"] += 1
            status = "âœ“ å®Œå…¨åŒ¹é… / Exact match"
        elif abs(time_diff) <= 5:
            results["close_match"] += 1
            status = f"âœ“ æ¥è¿‘ / Close (è¯¯å·® / error {time_diff:+d}ç§’ / seconds)"
        else:
            status = f"âœ— è¯¯å·® / Error ({time_diff:+d}ç§’ / seconds)"

        # ä»…æ‰“å°è¯¯å·®è¾ƒå¤§çš„æƒ…å†µ / Only print cases with large errors
        if abs(time_diff) > 5:
            print(f"\n[{source}] {aweme_id}")
            print(f"  å®é™…æ—¶é—´ / Actual time: {create_datetime} (ts: {actual_timestamp})")
            print(f"  è§£ç æ—¶é—´ / Decoded time: {decoded['datetime_utc']} (ts: {decoded_timestamp})")
            print(f"  {status}")

    # ç»Ÿè®¡æŠ¥å‘Š / Statistics report
    print("\n" + "=" * 80)
    print("=== éªŒè¯ç»“æœç»Ÿè®¡ / Validation Results Statistics ===")
    print("=" * 80)

    print(f"\nğŸ“Š æ€»ä½“å‡†ç¡®æ€§ / Overall Accuracy:")
    print(f"  éªŒè¯æ€»æ•° / Total validated:     {results['total']}")
    print(f"  å®Œå…¨åŒ¹é… / Exact match:     {results['exact_match']} ({results['exact_match']/results['total']*100:.1f}%)")
    print(f"  æ¥è¿‘åŒ¹é… / Close match:     {results['close_match']} ({results['close_match']/results['total']*100:.1f}%)")
    print(f"  è¾ƒå¤§è¯¯å·® / Large error:     {results['total'] - results['exact_match'] - results['close_match']}")

    # æ—¶é—´å·®åˆ†æ / Time difference analysis
    if results["time_diffs"]:
        avg_diff = sum(results["time_diffs"]) / len(results["time_diffs"])
        max_diff = max(results["time_diffs"])
        min_diff = min(results["time_diffs"])

        print(f"\nğŸ“ˆ è¯¯å·®åˆ†æ (ç»å¯¹å€¼) / Error Analysis (Absolute Value):")
        print(f"  å¹³å‡è¯¯å·® / Average error:     {avg_diff:.2f} ç§’ / seconds")
        print(f"  æœ€å¤§è¯¯å·® / Maximum error:     {max_diff} ç§’ / seconds")
        print(f"  æœ€å°è¯¯å·® / Minimum error:     {min_diff} ç§’ / seconds")

    # å¹³å°å¯¹æ¯” / Platform comparison
    if results["douyin_diffs"]:
        douyin_avg = sum(results["douyin_diffs"]) / len(results["douyin_diffs"])
        print(f"\nğŸ‡¨ğŸ‡³ æŠ–éŸ³å¹³å° / Douyin Platform:")
        print(f"  æ•°é‡ / Count:         {len(results['douyin_diffs'])}")
        print(f"  å¹³å‡è¯¯å·® / Average error:     {douyin_avg:+.2f} ç§’ / seconds")
        print(f"  è¯¯å·®èŒƒå›´ / Error range:     {min(results['douyin_diffs']):+d} ~ {max(results['douyin_diffs']):+d} ç§’ / seconds")

    if results["tiktok_diffs"]:
        tiktok_avg = sum(results["tiktok_diffs"]) / len(results["tiktok_diffs"])
        print(f"\nğŸŒ TikTokå¹³å° / TikTok Platform:")
        print(f"  æ•°é‡ / Count:         {len(results['tiktok_diffs'])}")
        print(f"  å¹³å‡è¯¯å·® / Average error:     {tiktok_avg:+.2f} ç§’ / seconds")
        print(f"  è¯¯å·®èŒƒå›´ / Error range:     {min(results['tiktok_diffs']):+d} ~ {max(results['tiktok_diffs']):+d} ç§’ / seconds")

    # ç»“è®º / Conclusion
    print("\n" + "=" * 80)
    print("=== ç»“è®º / Conclusion ===")
    print("=" * 80)

    accuracy_rate = (results["exact_match"] + results["close_match"]) / results["total"] * 100

    if accuracy_rate >= 95:
        conclusion = "âœ… ç®—æ³•å‡†ç¡®æ€§æé«˜ï¼Œé«˜32ä½ç¡®å®æ˜¯Unixæ—¶é—´æˆ³ï¼ˆç§’çº§ï¼‰/ Algorithm is highly accurate, high 32 bits are indeed Unix timestamp (second-level)"
    elif accuracy_rate >= 80:
        conclusion = "âœ… ç®—æ³•å‡†ç¡®æ€§è¾ƒé«˜ï¼Œå­˜åœ¨å°å¹…æ—¶é—´åç§» / Algorithm accuracy is good, with minor time offset"
    else:
        conclusion = "âš ï¸  ç®—æ³•å­˜åœ¨è¾ƒå¤§è¯¯å·®ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ / Algorithm has significant errors, needs further analysis"

    print(f"\n{conclusion}")
    print(f"å‡†ç¡®ç‡ï¼ˆâ‰¤5ç§’è¯¯å·®ï¼‰/ Accuracy (â‰¤5 sec error): {accuracy_rate:.1f}%")

    print("\nğŸ’¡ è§£è¯» / Interpretation:")
    print("  â€¢ é«˜32ä½ / High 32 bits = Unixæ—¶é—´æˆ³ï¼ˆç§’çº§ï¼‰/ Unix timestamp (second-level)")
    print("  â€¢ ä½32ä½ / Low 32 bits = åˆ†ç‰‡ID + åºåˆ—å· / Shard ID + Sequence number")
    print("  â€¢ è¯¯å·®å¯èƒ½æ¥æº / Possible error sourcesï¼šæœåŠ¡å™¨æ—¶é’Ÿåç§»ã€æ—¶åŒºè½¬æ¢ã€è§†é¢‘ä¸Šä¼ å»¶è¿Ÿ / Server clock offset, timezone conversion, video upload delay")
    print("  â€¢ ç®—æ³•å¯ç”¨äº / Algorithm can be used forï¼šæ—¶é—´èŒƒå›´æŸ¥è¯¢ã€æ—¶åºåˆ†æã€åˆ†å¸ƒå¼è¿½è¸ª / Time range query, time series analysis, distributed tracing")


def visualize_bit_structure(aweme_id: str) -> None:
    """
    å¯è§†åŒ–å±•ç¤ºIDçš„ä½ç»“æ„
    Visualize ID bit structure
    """
    print("\n" + "=" * 80)
    print("=== IDä½ç»“æ„å¯è§†åŒ– / ID Bit Structure Visualization ===")
    print("=" * 80)

    result = decode_aweme_id(aweme_id, analyze_low32=True)
    n = int(aweme_id)

    # 64ä½äºŒè¿›åˆ¶è¡¨ç¤º / 64-bit binary representation
    bin_str = format(n, '064b')

    print(f"\nID: {aweme_id}")
    print(f"\nå®Œæ•´64ä½äºŒè¿›åˆ¶ / Complete 64-bit Binary:")
    print("â”Œ" + "â”€" * 32 + "â”¬" + "â”€" * 32 + "â”")
    print("â”‚" + " " * 8 + "é«˜32ä½ (æ—¶é—´æˆ³)" + " " * 8 + "â”‚" + " " * 8 + "ä½32ä½ (å”¯ä¸€æ€§)" + " " * 8 + "â”‚")
    print("â”‚" + " " * 6 + "High 32 bits (Timestamp)" + " " * 6 + "â”‚" + " " * 6 + "Low 32 bits (Uniqueness)" + " " * 6 + "â”‚")
    print("â”œ" + "â”€" * 32 + "â”¼" + "â”€" * 32 + "â”¤")

    # åˆ†æ®µæ˜¾ç¤º / Segmented display
    high32 = bin_str[:32]
    low32 = bin_str[32:]

    # æ¯8ä½ä¸€ç»„æ˜¾ç¤º / Display in groups of 8 bits
    print("â”‚ ", end="")
    for i in range(0, 32, 8):
        print(high32[i:i+8], end=" ")
    print("â”‚ ", end="")
    for i in range(0, 32, 8):
        print(low32[i:i+8], end=" ")
    print("â”‚")

    print("â””" + "â”€" * 32 + "â”´" + "â”€" * 32 + "â”˜")

    # è§£æç»“æœ / Parse result
    ts_sec = n >> 32
    low32_val = n & 0xFFFFFFFF

    print(f"\né«˜32ä½è§£æ / High 32-bit Parsing:")
    print(f"  äºŒè¿›åˆ¶ / Binary: {high32}")
    print(f"  åè¿›åˆ¶ / Decimal: {ts_sec}")
    print(f"  æ—¶é—´ / Time:   {result['datetime_utc']}")

    print(f"\nä½32ä½è§£æ (æ¨èæ–¹æ¡ˆ: 16+16) / Low 32-bit Parsing (Recommended: 16+16):")
    analysis = result["low32_analysis"]["scheme3_16_16"]

    low32_bin = format(low32_val, '032b')
    shard_bin = low32_bin[:16]
    seq_bin = low32_bin[16:]

    print("â”Œ" + "â”€" * 16 + "â”¬" + "â”€" * 16 + "â”")
    print("â”‚  åˆ†ç‰‡ID (16ä½) â”‚  åºåˆ—å· (16ä½) â”‚")
    print("â”‚  Shard ID (16) â”‚ Sequence (16) â”‚")
    print("â”œ" + "â”€" * 16 + "â”¼" + "â”€" * 16 + "â”¤")
    print(f"â”‚ {shard_bin[:8]} {shard_bin[8:]} â”‚ {seq_bin[:8]} {seq_bin[8:]} â”‚")
    print("â””" + "â”€" * 16 + "â”´" + "â”€" * 16 + "â”˜")

    print(f"  åˆ†ç‰‡ID / Shard ID: {analysis['shard_id']:6d} (0x{analysis['shard_id']:04x})")
    print(f"  åºåˆ—å· / Sequence: {analysis['sequence']:6d} (0x{analysis['sequence']:04x})")

    print(f"\nè§£è¯» / Interpretation:")
    print(f"  âœ“ è§†é¢‘åœ¨ / Video published at {result['datetime_utc']}")
    print(f"  âœ“ ç”±åˆ†ç‰‡/æœåŠ¡å™¨ / Processed by shard/server #{analysis['shard_id']}")
    print(f"  âœ“ è¯¥ç§’å†…çš„åºåˆ—å·ä¸º / Sequence number within that second is #{analysis['sequence']}")


if __name__ == "__main__":
    print("=" * 80)
    print("æŠ–éŸ³ / TikTok è§†é¢‘ ID è§£å¯†å·¥å…·")
    print("Douyin / TikTok Video ID Decoder")
    print("ä½œè€… / Author: Evil0ctal")
    print("=" * 80)

    # ==================== ğŸ”¥ é‡ç‚¹ï¼šç®—æ³•éªŒè¯ / Focus: Algorithm Validation ====================
    # ä½¿ç”¨çœŸå®æ•°æ®éªŒè¯è§£ç ç®—æ³•çš„å‡†ç¡®æ€§ / Validate decode algorithm accuracy using real data
    print("\n" + "ğŸ¯" * 40)
    print("ä½¿ç”¨ aweme_ids_output.json è¿›è¡Œç®—æ³•éªŒè¯")
    print("Using aweme_ids_output.json for algorithm validation")
    print("ğŸ¯" * 40)

    validate_decode_algorithm()

    # ==================== å…¶ä»–åŠŸèƒ½æ¼”ç¤º / Other Feature Demonstrations ====================
    # ç¤ºä¾‹è¾“å…¥ï¼ˆTikTok / æŠ–éŸ³è§†é¢‘IDï¼‰/ Sample input (TikTok / Douyin video IDs)
    sample_ids = [
        # Douyin aweme_ids and create_time
        "7153549929326120227",  # 1665565640
        "7266740902494833931",  # 1691919969
        "7196618597496524067",  # 1675593344
        # TikTok aweme_ids and create_time
        "7559684939684400414",  # 1760126333
        "7559661864628538654",  # 1760120961
        "7559607368695188766",  # 1760108270
    ]

    # å¹³å°æ ‡ç­¾ / Platform labels
    platform_labels = [
        "Douyin-1", "Douyin-2", "Douyin-3",
        "TikTok-1", "TikTok-2", "TikTok-3"
    ]

    # 1. åŸºç¡€è§£ç  / Basic decoding
    print("\n=== æ­¥éª¤1: åŸºç¡€è§£ç  / Step 1: Basic Decoding ===")
    batch_decode(sample_ids)

    # 2. æ·±åº¦åˆ†æä½32ä½ç»“æ„ / Deep analysis of low 32-bit structure
    deep_analyze_low32(sample_ids)

    # 3. ç»Ÿè®¡åˆ†æ / Statistical analysis
    statistical_analysis(sample_ids, platform_labels)

    # 4. å¯è§†åŒ–å•ä¸ªID / Visualize a single ID
    # visualize_bit_structure(sample_ids[0])

    # 5. ç¤ºä¾‹ï¼šåå‘æ„é€ ä¸€ä¸ªæ–°ID / Example: Forge a new ID
    print("\n" + "=" * 80)
    print("=== æ„é€ ä¸€æšè‡ªå®šä¹‰ç±»æŠ–éŸ³ID / Forge a Custom Douyin-style ID ===")
    print("=" * 80)
    now_ts = int(datetime.now(tz=timezone.utc).timestamp())
    fake_id = forge_aweme_like_id(now_ts)
    print(f"å½“å‰UTCæ—¶é—´æˆ³ / Current UTC timestamp: {now_ts}")
    print(f"ä¼ªé€ çš„ID / Forged ID: {fake_id}")
    print("\néªŒè¯è§£ç ç»“æœ / Verify decoding result:")
    decoded_result = decode_aweme_id(str(fake_id), analyze_low32=True)
    for key, value in decoded_result.items():
        if key != "low32_analysis":
            print(f"  {key}: {value}")
        else:
            print(f"\n  ä½32ä½åˆ†æ / Low 32-bit analysis:")
            for scheme, data in value.items():
                print(f"    {scheme}: {data}")
